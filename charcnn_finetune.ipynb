{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_fake = pd.read_csv(\"data/Fake.csv\")\n",
    "df_real = pd.read_csv(\"data/True.csv\")\n",
    "df_fake[\"label\"] = 1\n",
    "df_real[\"label\"] = 0\n",
    "df_main = pd.concat([df_fake, df_real], ignore_index=True)[[\"text\", \"label\"]].dropna()\n",
    "\n",
    "df_fake2 = pd.read_csv(\"data/fine_tune_data/ISOT_Fake.csv\")\n",
    "df_real2 = pd.read_csv(\"data/fine_tune_data/ISOT_True.csv\")\n",
    "\n",
    "df_fake2[\"label\"] = 1\n",
    "df_real2[\"label\"] = 0\n",
    "df_isot = pd.concat([df_fake2, df_real2], ignore_index=True)[[\"text\", \"label\"]].dropna()\n",
    "\n",
    "df_liar = pd.read_csv(\"data/liar/train.tsv\", sep='\\t', header=None)\n",
    "df_liar.columns = [\n",
    "    \"id\", \"label\", \"statement\", \"subject\", \"speaker\", \"job_title\", \"state_info\",\n",
    "    \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\",\n",
    "    \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"\n",
    "]\n",
    "true_labels = [\"true\", \"half-true\", \"mostly-true\"]\n",
    "false_labels = [\"false\", \"barely-true\", \"pants-fire\"]\n",
    "df_liar = df_liar[df_liar[\"label\"].isin(true_labels + false_labels)].copy()\n",
    "df_liar[\"label\"] = df_liar[\"label\"].apply(lambda x: 1 if x in false_labels else 0)\n",
    "df_liar = df_liar[[\"statement\", \"label\"]].rename(columns={\"statement\": \"text\"}).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "char_vocab = {ch: idx + 1 for idx, ch in enumerate(\"abcdefghijklmnopqrstuvwxyz0123456789 .,;!?-‚Äì()[]{}'\\\"\")}\n",
    "vocab_size = len(char_vocab) + 1 \n",
    "MAX_LEN = 1014  \n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 .,;!?()\\\\[\\\\]{}\\'\"-]', '', text)\n",
    "    return text\n",
    "\n",
    "def text_to_sequence(text, max_len=MAX_LEN):\n",
    "    text = clean_text(text)\n",
    "    seq = [char_vocab.get(c, 0) for c in text[:max_len]]\n",
    "    return seq + [0] * (max_len - len(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sequences = dataset[\"text\"].apply(text_to_sequence).tolist()\n",
    "labels = dataset[\"label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sequences, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_classes=2):\n",
    "        super(CharCNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 256, kernel_size=7, padding=3)\n",
    "        self.pool1 = nn.MaxPool1d(3)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(256, 256, kernel_size=7, padding=3)\n",
    "        self.pool2 = nn.MaxPool1d(3)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(3)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 34, 1024)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)          \n",
    "        x = x.permute(0, 2, 1)        \n",
    "\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool3(F.relu(self.conv5(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"base_model.pt\")\n",
    "print(\"‚úÖ ƒ∞lk eƒüitim tamamlandƒ±, model kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdogaozyagci\u001b[0m (\u001b[33mveyselbayrakci-isik-universitesi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/doga/charcnn_finetune/wandb/run-20250531_153651-u0qbnlg4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/veyselbayrakci-isik-universitesi/charcnn-finetune/runs/u0qbnlg4' target=\"_blank\">charcnn-run-isot</a></strong> to <a href='https://wandb.ai/veyselbayrakci-isik-universitesi/charcnn-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/veyselbayrakci-isik-universitesi/charcnn-finetune' target=\"_blank\">https://wandb.ai/veyselbayrakci-isik-universitesi/charcnn-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/veyselbayrakci-isik-universitesi/charcnn-finetune/runs/u0qbnlg4' target=\"_blank\">https://wandb.ai/veyselbayrakci-isik-universitesi/charcnn-finetune/runs/u0qbnlg4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"charcnn-finetune\", name=\"charcnn-run-isot\", config={\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.001\n",
    "})\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "model = CharCNN(vocab_size=config.vocab_size).to(\"cpu\") \n",
    "model.load_state_dict(torch.load(\"base_model.pt\"))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "def train_model(model, loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        avg_loss = total_loss / len(loader)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"train_accuracy\": acc,\n",
    "            \"learning_rate\": config.learning_rate\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, epochs=config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"üîç Test Loss: {avg_loss:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"test_loss\": avg_loss,\n",
    "        \"test_accuracy\": acc\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_isot.copy()\n",
    "\n",
    "sequences = dataset[\"text\"].apply(text_to_sequence).tolist()\n",
    "labels = dataset[\"label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.1, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"charcnn-finetune\", name=\"charcnn-run-isot\", config={\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.0001\n",
    "})\n",
    "\n",
    "model = CharCNN(vocab_size=vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix_isot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"confusion_matrix\": wandb.Image(\"confusion_matrix_isot.png\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"charcnn_finetuned_isot.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
